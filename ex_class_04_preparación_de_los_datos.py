# -*- coding: utf-8 -*-
"""EX_CLASS_04_Preparación_de_los_datos.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18_GxqD1Y5rECFpEcQ46UWjqra-u4s7-n

#Clase: Preparación de los Datos
En base a lo entregado por el profesor: **Christian Alvarez Carreño**

#1.-Imputación de datos

Imputación de valores usando el **promedio**
"""

import pandas as pd
import numpy as np

# Creamos un DataFrame de ejemplo
df = pd.DataFrame({'edad': [25, 30, 35, np.nan, 40, np.nan, 50]})

# Imputamos los valores faltantes con la media de la columna "edad"
media_edad = df['edad'].mean()
df['edad'].fillna(media_edad, inplace=True)

# Imprimimos el DataFrame actualizado
print(df)

"""**Imputación de valores usando la moda**"""

import pandas as pd

# Creamos un DataFrame de ejemplo
df = pd.DataFrame({'género': ['Hombre', 'Mujer', 'Hombre', 'Hombre', None, None, 'Mujer', 'Mujer']})

# Imputamos los valores faltantes con la moda de la columna "género"
moda_genero = df['género'].mode()[0]
df['género'].fillna(moda_genero, inplace=True)

# Imprimimos el DataFrame actualizado
print(df)

"""**Imputando valores usando regresión lineal**

Utilizamos el modelo entrenado para predecir los valores faltantes de "y" utilizando los valores de "x" de los datos incompletos.
"""

import pandas as pd
from sklearn.linear_model import LinearRegression

# Creamos un DataFrame de ejemplo
df = pd.DataFrame({'x': [1, 2, 3, 4, 5, 6, 7, 8, 9],
                   'y': [2, 4, 6, None, 10, None, 14, 16, 18]})

# Separamos los datos completos y los incompletos
datos_completos = df.dropna()
datos_incompletos = df[df.isna().any(axis=1)]

# Creamos un modelo de regresión lineal
modelo = LinearRegression()

# Entrenamos el modelo con los datos completos
X = datos_completos['x'].values.reshape(-1, 1)
y = datos_completos['y'].values.reshape(-1, 1)
modelo.fit(X, y)

# Imputamos los valores faltantes utilizando el modelo
X_incompletos = datos_incompletos['x'].values.reshape(-1, 1)
y_inferidos = modelo.predict(X_incompletos)
df.loc[df['y'].isnull(), 'y'] = y_inferidos

# Imprimimos el DataFrame actualizado
print(df)

"""**KNN Imputer**

*Explicación de los hiperparámetros:*

**n_neighbors:** Este parámetro establece el número de vecinos que se utilizarán para calcular los valores imputados. Por ejemplo, si n_neighbors es 2, el imputador buscará los 2 vecinos más cercanos y promediará sus valores para imputar el valor faltante. Cuanto mayor sea este valor, más vecinos se tendrán en cuenta en el cálculo.

**weights:** Este parámetro determina cómo se ponderarán los valores de los vecinos al calcular el valor imputado. Las opciones comunes son 'uniform' (todos los vecinos tienen el mismo peso) y 'distance' (los vecinos más cercanos tienen un peso mayor). Si se establece en 'uniform', el imputador simplemente promediará los valores de los vecinos. Si se establece en 'distance', los valores de los vecinos se ponderarán por la inversa de la distancia.
"""

import numpy as np
from sklearn.impute import KNNImputer

# Datos de ejemplo con valores faltantes
data = np.array([[1, 2, np.nan],
                 [4, np.nan, 6],
                 [7, 8, 9]])

# Crear una instancia de KNNImputer
imputer = KNNImputer(n_neighbors=2, weights='uniform')

# Imputar los valores faltantes
imputed_data = imputer.fit_transform(data)

print("Datos originales:")
print(data)
print("Datos imputados:")
print(imputed_data)

"""**MICE Imputer**

*Explicación de los hiperparámetros:*

**max_iter**: Este parámetro establece el número máximo de iteraciones que el algoritmo realizará para imputar los valores faltantes. Cada iteración es un paso donde las características faltantes se imputan utilizando un modelo ajustado a las otras características.

**random_state**: Este parámetro se utiliza para controlar la aleatoriedad en el proceso de imputación. Puedes establecer un número para asegurarte de obtener resultados consistentes en diferentes ejecuciones.
"""

pip install fancyimpute

import numpy as np
from fancyimpute import IterativeImputer

# Datos de ejemplo con valores faltantes
data = np.array([[1, 2, np.nan],
                 [4, np.nan, 6],
                 [7, 8, 9]])

# Crear una instancia de IterativeImputer
imputer = IterativeImputer(max_iter=10, random_state=0)

# Imputar los valores faltantes
imputed_data = imputer.fit_transform(data)

print("Datos originales:")
print(data)
print("Datos imputados:")
print(imputed_data)

"""#2.-Transformación de datos categóricos

**Usando Label encoding**
"""

import pandas as pd
from sklearn.preprocessing import LabelEncoder

# Creamos un DataFrame de ejemplo
df = pd.DataFrame({'tamaño': ['pequeño', 'mediano', 'grande', 'mediano', 'pequeño', 'grande', 'grande']})

# Creamos un objeto LabelEncoder
le = LabelEncoder()

# Definimos el orden de las categorías
categorias = ['pequeño', 'mediano','grande']

# Ajustamos y transformamos la columna "tamaño"
le.fit(categorias)
df['tamaño_codificado'] = le.transform(df['tamaño'])

# Imprimimos el DataFrame actualizado
print(df)

"""Ahora veamos la inversa de la decodificación"""

# Invertir la transformación usando inverse_transform
df['tamaño_decodificado'] = le.inverse_transform(df['tamaño_codificado'])

# Imprimimos el DataFrame con la columna decodificada
print("DataFrame con decodificación:")
print(df)

"""**Usando Mapeos** para que quede asignado de la manera que nosotros queramos."""

import pandas as pd

# Creamos un DataFrame de ejemplo
df = pd.DataFrame({
    'tamaño': ['mediano', 'pequeño', 'grande', 'pequeño', 'mediano', 'grande', 'grande']
})

# Creamos un diccionario de datos que asocia las categorías con sus valores codificados correspondientes
codificacion = {'pequeño': 0, 'mediano': 1, 'grande': 2}

# Codificamos la columna "tamaño" utilizando el diccionario de datos
df['tamaño_codificado'] = df['tamaño'].map(codificacion)

# Imprimimos el DataFrame actualizado
print(df)

# Crear un diccionario inverso para la decodificación
decodificacion = {v: k for k, v in codificacion.items()}

# Decodificar la columna "tamaño_codificado" usando el diccionario inverso
df['tamaño_decodificado'] = df['tamaño_codificado'].map(decodificacion)

# Imprimimos el DataFrame con la columna decodificada
print("DataFrame con decodificación:")
print(df)

"""Usar **One Hot Encoding** para variables categóricas nominales"""

import pandas as pd

# Creamos un DataFrame de ejemplo
df = pd.DataFrame({
    'color': ['red', 'green', 'blue', 'red']
})

# Utilizamos el método get_dummies() de Pandas para realizar One-Hot Encoding de la columna "color"
one_hot = pd.get_dummies(df['color'])

# Concatenamos el DataFrame original con el DataFrame codificado
df = pd.concat([df, one_hot], axis=1)

# Imprimimos el DataFrame actualizado
print(df)

# Obtenemos la columna original a partir del One Hot Encoding
columna_original = one_hot.idxmax(axis=1)

# Agregamos la columna original al DataFrame
df['color_original'] = columna_original

# Imprimimos el DataFrame con la columna original recuperada
print("DataFrame con columna original recuperada:")
print(df)

"""#3.-Transformación de Datos Numéricos

**Normalizando los datos [0,1] con MixMaxScaler**
"""

from sklearn.preprocessing import MinMaxScaler
import numpy as np

# Crear conjunto de datos de ejemplo
datos = np.array([[25, 50000],
                  [30, 60000],
                  [35, 70000],
                  [40, 80000]])

# Crear objeto scaler
scaler = MinMaxScaler()

# Escalar las variables
datos_esc = scaler.fit_transform(datos)

# Ver los datos escalados
print(datos_esc)

# Inversa del escalamiento
datos_originales = scaler.inverse_transform(datos_esc)

# Imprimir los datos originales
np.set_printoptions(suppress=True) #suprime muestra en potencia
print("Datos originales:")
print(datos_originales)

"""**Estandarizando los datos con StandarScaler** (con la media en 0 y la desviación estándar = 1)"""

from sklearn.preprocessing import StandardScaler
import numpy as np

# Crear conjunto de datos de ejemplo
datos = np.array([[25, 50000],
                  [30, 60000],
                  [35, 70000],
                  [40, 80000]])

# Crear objeto scaler
scaler = StandardScaler()

# Escalar las variables
datos_esc = scaler.fit_transform(datos)

# Ver los datos escalados
print(datos_esc)

# Inversa de la estandarización
datos_originales = scaler.inverse_transform(datos_esc)

# Imprimir los datos originales
print("Datos originales:")
print(datos_originales)

"""#4.-Conservando Transformaciones


"""

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression

# Cargar el conjunto de datos Iris
from sklearn.datasets import load_iris
iris = load_iris()
X = iris.data
y = iris.target

# Dividir el conjunto de datos en entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Escalar los datos
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Entrenar el modelo de clasificación (Regresión Logística)
model = LogisticRegression(max_iter=1000)
model.fit(X_train_scaled, y_train)

# Probar con un nuevo registro idéntico al dataset original
new_data = np.array([[5.1, 3.5, 1.4, 0.2]])  # Nuevo registro de ejemplo
new_data_scaled = scaler.transform(new_data)  # Escalar el nuevo registro
predicted_class = model.predict(new_data_scaled)

print("Nuevo registro:")
print("Características:", new_data)
print("Clase predicha:", predicted_class[0])

"""**Exportando el Scaler**"""

import pickle

# Guardar el scaler en un archivo
with open('scaler.pkl', 'wb') as f:
    pickle.dump(scaler, f)

# Cargar el scaler desde el archivo
with open('scaler.pkl', 'rb') as f:
    loaded_scaler = pickle.load(f)

# Probar con un nuevo registro idéntico al dataset original
new_data = np.array([[4.5, 6.2, 1.8, 0.6]])  # Nuevo registro de ejemplo
new_data_scaled = loaded_scaler.transform(new_data)  # Escalar el nuevo registro
predicted_class = model.predict(new_data_scaled)

print("Nuevo registro:")
print("Características:", new_data)
print("Clase predicha:", predicted_class[0])

"""#Desafío

Imputa los datos faltantes de este set de datos categórico.
"""

data = {
    'color': ['negro', 'blanco', 'gris', 'negro', 'blanco', 'gris', np.nan, 'gris', 'blanco', 'negro', 'gris', 'blanco', 'gris', 'negro', 'blanco', 'negro', 'blanco', 'gris', 'negro', 'blanco', 'gris', 'negro', 'blanco', 'gris', 'negro'],
    'tamaño': ['pequeño', 'mediano', 'grande', 'pequeño', 'mediano', 'grande', 'pequeño', 'grande', np.nan, 'pequeño', 'grande', 'mediano', 'grande', 'pequeño', 'mediano', 'pequeño', 'grande', 'mediano', 'pequeño', 'grande', 'mediano', 'pequeño', 'grande', 'mediano', 'pequeño'],
    'raza': ['siames', np.nan, 'angora', 'siames', 'persa', 'angora', 'siames', 'angora', 'persa', 'siames', 'angora', 'persa', 'siames', 'angora', 'persa', 'siames', 'angora', 'persa', 'siames', 'angora', 'persa', 'siames', 'angora', 'persa', 'siames']
}

print(data)

df = pd.DataFrame(data)
df

# Definir las categorías para cada columna
categorias = {
    'tamaño': ['pequeño', 'mediano', 'grande'],
    'raza': ['siames', 'persa', 'angora'],
    'color': ['blanco', 'gris', 'negro']
}

# Crear un LabelEncoder para cada columna y transformar los valores
label_encoders = {}
for columna, cats in categorias.items():
    le = LabelEncoder()
    # Ignorar np.nan durante la transformación
    valores_no_nan = [valor for valor in df[columna] if pd.notnull(valor)]
    le.fit(valores_no_nan)
    label_encoders[columna] = le
    df[columna] = df[columna].apply(lambda x: le.transform([x])[0] if pd.notnull(x) else x)

print(df)

# Utilizar KNNImputer para rellenar los valores np.nan
imputer = KNNImputer(n_neighbors=3)  # Puedes ajustar el número de vecinos según tu necesidad
df_imputed = imputer.fit_transform(df)


# Crear un nuevo DataFrame con los valores imputados
df_imputed = pd.DataFrame(df_imputed, columns=df.columns)

print(df_imputed)

for columna in categorias.keys():
    df_imputed[columna] = df_imputed[columna].apply(lambda x: label_encoders[columna].inverse_transform([int(x)])[0] if pd.notnull(x) else x)

print(df_imputed)